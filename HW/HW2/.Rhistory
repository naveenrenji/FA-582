"Pre-Tax ROE", "Profit Margin", "Quick Ratio", "Total Assets", "Total Liabilities", "Earnings Per Share")
fundamentals_2013 <- fundamentals_2013 %>%
rowwise() %>%
mutate(missing_or_zero = sum(across(all_of(quantitative_cols), ~is.na(.) | . == 0), na.rm = TRUE)) %>%
ungroup()
# Sorting the DataFrame based on the missing_or_zero column and then picking the first 100 tickers.
top_100_tickers <- fundamentals_2013 %>%
arrange(missing_or_zero) %>%
head(100) %>%
pull('Ticker Symbol')
# Selecting rows for the top 100 tickers
fundamentals_2013_subset <- filter(fundamentals_2013, `Ticker Symbol` %in% top_100_tickers)
securities_subset <- filter(securities_df, `Ticker symbol` %in% top_100_tickers)
# Select only the quantitative columns of interest from fundamentals_2013_subset
selected_columns <- c("Ticker Symbol", "After Tax ROE", "Cash Ratio", "Current Ratio", "Operating Margin", "Pre-Tax Margin", "Pre-Tax ROE", "Profit Margin", "Quick Ratio", "Total Assets", "Total Liabilities", "Earnings Per Share")
fundamentals_2013_subset_selected <- dplyr::select(fundamentals_2013_subset, all_of(selected_columns))
# Merge the two data frames
merged_df <- merge(fundamentals_2013_subset_selected, securities_subset, by.x = "Ticker Symbol", by.y = "Ticker symbol")
df_numeric <- merged_df[, c("After Tax ROE", "Cash Ratio", "Current Ratio", "Operating Margin", "Pre-Tax Margin",
"Pre-Tax ROE", "Profit Margin", "Quick Ratio", "Total Assets", "Total Liabilities", "Earnings Per Share")]
df_categorical <- merged_df[, c("Security", "SEC filings", "GICS Sector","GICS Sub Industry", "Address of Headquarters")]
# Define the function
compute_normalized_similarity <- function(df_numeric, df_categorical, lambda) {
# Function to calculate numerical similarity (e.g., Euclidean distance)
calc_num_sim <- function(x, y, sigma_n) {
return(sum(((x - y)^2) / sigma_n^2))
}
# Function to calculate categorical similarity (e.g., Jaccard similarity)
calc_cat_sim <- function(x, y, sigma_c) {
return(sum(x == y) / sigma_c)
}
# Get the number of tickers
num_tickers <- nrow(df_numeric)
ticker_symbols <- merged_df$`Ticker Symbol`
# Initialize an empty matrix to store overall similarity values
overall_sim_df <- data.frame(Var1 = character(), Var2 = character(), value = numeric())
overall_sim_matrix <- matrix(0, nrow=num_tickers, ncol=num_tickers)
rownames(overall_sim_matrix) <- ticker_symbols
colnames(overall_sim_matrix) <- ticker_symbols
# Standard deviations for normalization
sigma_n <- apply(df_numeric, 2, sd, na.rm = TRUE)  # column-wise standard deviation
sigma_c <- length(unique(as.vector(as.matrix(df_categorical))))  # unique categories for normalization
# Calculate overall similarity
for (i in 1:(num_tickers - 1)) {
for (j in (i + 1):num_tickers) {
num_sim <- calc_num_sim(df_numeric[i, ], df_numeric[j, ], sigma_n)
cat_sim <- calc_cat_sim(df_categorical[i, ], df_categorical[j, ], sigma_c)
overall_sim <- lambda * num_sim + (1 - lambda) * cat_sim
overall_sim_df <- rbind(overall_sim_df, data.frame(Var1 = ticker_symbols[i], Var2 = ticker_symbols[j], value = overall_sim))
}
}
# Rank the similarities
overall_sim_df$rank <- rank(-overall_sim_df$value)  # - sign to rank highest similarities first
# Extract top and bottom 10 similarities
top_10_sim <- overall_sim_df[order(-overall_sim_df$value), ][1:10,]
bottom_10_sim <- overall_sim_df[order(overall_sim_df$value), ][1:10,]
# Return a list containing top and bottom 10 similarities
return(list(top_10 = top_10_sim, bottom_10 = bottom_10_sim))
}
# Assuming lambda = 0.5 for equal weight to numerical and categorical features
lambda <- 0.1
# Get top and bottom 10 similarities
similarity_results <- compute_normalized_similarity(df_numeric, df_categorical, lambda)
# Print results
print("Top 10 similarities:")
print(similarity_results$top_10)
print("Bottom 10 similarities:")
print(similarity_results$bottom_10)
# Load the required libraries
library(readr)
library(dplyr)
library(reshape2)
library(tidyr)
# Load the data from the CSV files
fundamentals_df <- read_csv("fundamentals.csv")
quantitative_cols <- c("After Tax ROE", "Cash Ratio", "Current Ratio", "Operating Margin", "Pre-Tax Margin",
"Pre-Tax ROE", "Profit Margin", "Quick Ratio", "Total Assets", "Total Liabilities", "Earnings Per Share")
# Filter Data for Specific Year (2013)
fundamentals_2013 <- filter(fundamentals_df, substr(`Period Ending`, 1, 4) == "2013")
fundamentals_2013 <- fundamentals_2013 %>%
rowwise() %>%
mutate(missing_or_zero = sum(across(all_of(quantitative_cols), ~is.na(.) | . == 0), na.rm = TRUE)) %>%
ungroup()
# Sorting the DataFrame based on the missing_or_zero column and then picking the first 100 tickers.
top_100_tickers <- fundamentals_2013 %>%
arrange(missing_or_zero) %>%
head(100) %>%
pull('Ticker Symbol')
# Selecting rows for the top 100 tickers
fundamentals_2013_subset <- filter(fundamentals_2013, `Ticker Symbol` %in% top_100_tickers)
# selected columns
fundamentals_2013_subset_selected <- select(fundamentals_2013_subset, one_of(c("Ticker Symbol", quantitative_cols)))
# Load the required libraries
library(readr)
library(dplyr)
library(reshape2)
library(tidyr)
# Load the data from the CSV files
fundamentals_df <- read_csv("fundamentals.csv")
quantitative_cols <- c("After Tax ROE", "Cash Ratio", "Current Ratio", "Operating Margin", "Pre-Tax Margin",
"Pre-Tax ROE", "Profit Margin", "Quick Ratio", "Total Assets", "Total Liabilities", "Earnings Per Share")
# Filter Data for Specific Year (2013)
fundamentals_2013 <- filter(fundamentals_df, substr(`Period Ending`, 1, 4) == "2013")
fundamentals_2013 <- fundamentals_2013 %>%
rowwise() %>%
mutate(missing_or_zero = sum(across(all_of(quantitative_cols), ~is.na(.) | . == 0), na.rm = TRUE)) %>%
ungroup()
# Sorting the DataFrame based on the missing_or_zero column and then picking the first 100 tickers.
top_100_tickers <- fundamentals_2013 %>%
arrange(missing_or_zero) %>%
head(100) %>%
pull('Ticker Symbol')
# Selecting rows for the top 100 tickers
fundamentals_2013_subset <- filter(fundamentals_2013, `Ticker Symbol` %in% top_100_tickers)
# selected columns
selected_columns <- c("Ticker Symbol", "After Tax ROE", "Cash Ratio", "Current Ratio", "Operating Margin", "Pre-Tax Margin", "Pre-Tax ROE", "Profit Margin", "Quick Ratio", "Total Assets", "Total Liabilities", "Earnings Per Share")
fundamentals_2013_subset_selected <- dplyr::select(fundamentals_2013_subset, all_of(selected_columns))
# Function for Lp-norm
lpnorm <- function(x, y, p) {
return (sum(abs(x - y)^p)^(1 / p))
}
# Function for weighted Minkowski distance
weights <- c(0.0941, 0.0941, 0.0941, 0.0824, 0.0706, 0.0706, 0.1059, 0.1059, 0.0941, 0.0588) #calculation explained in the report
weighted_minkowski <- function(x, y, p) {
return (sum(weights * abs(x - y)^p)^(1 / p))
}
# Calculate distances and sort and return as data frame
calc_and_sort_distances <- function(df, dist_func, p = NULL) {
dist_list <- list()
for (i in 1:(nrow(df) - 1)) {
for (j in (i + 1):nrow(df)) {
ticker1 <- as.character(df[i, 'Ticker Symbol'])
ticker2 <- as.character(df[j, 'Ticker Symbol'])
pair <- paste(ticker1, ticker2, sep = "-")
if (is.null(p)) {
dist <- dist_func(df[i, -1], df[j, -1])
} else {
dist <- dist_func(df[i, -1], df[j, -1], p)
}
dist_list[[pair]] <- dist
}
}
sorted_list <- sort(unlist(dist_list))
sorted_df <- data.frame(Ticker_Pair = names(sorted_list), Distance = sorted_list)
return (sorted_df)
}
# Lp-norm calculations
for (p in c(1, 2, 3, 10)) {
sorted_distances_df <- calc_and_sort_distances(fundamentals_2013_subset_selected, lpnorm, p)
print(paste("Top 10 and Bottom 10 for Lp-norm with p =", p))
print("Top 10:")
print(head(sorted_distances_df, 10))
print("Bottom 10:")
print(tail(sorted_distances_df, 10))
}
# Minkowski distance calculations
sorted_distances_minkowski_df <- calc_and_sort_distances(fundamentals_2013_subset_selected, weighted_minkowski, 2)
print("Top 10 and Bottom 10 for Weighted Minkowski Distance")
print("Top 10:")
print(head(sorted_distances_minkowski_df, 10))
print("Bottom 10:")
print(tail(sorted_distances_minkowski_df, 10))
library(dplyr)
library(readr)
library(tidyr)
# Read the data
securities_df <- read_csv("securities.csv")
categorical_cols <- c("GICS Sector", "GICS Sub Industry", "Address of Headquarters")
# Preprocess the data
securities_df <- securities_df %>%
rowwise() %>%
mutate(missing_or_zero = sum(across(all_of(categorical_cols), ~is.na(.) | . == 0), na.rm = TRUE)) %>%
ungroup()
# Select top 100 tickers
top_100_tickers <- securities_df %>%
arrange(missing_or_zero) %>%
head(100) %>%
pull('Ticker symbol')
securities_subset <- filter(securities_df, `Ticker symbol` %in% top_100_tickers)
# Convert all categorical columns to character type
securities_subset <- securities_subset %>%
mutate(across(all_of(categorical_cols), as.character))
# overlap similarity
overlap_similarity <- function(x, y) {
if (is.na(x) || is.na(y)) return(0)
if (x == y) return(1)
return(0)
}
# inverse frequency similarity
inverse_frequency_similarity <- function(x, y, p) {
if (is.na(x) || is.na(y)) return(0)
if (x == y) return(1 / (p^2))
return(0)
}
#Goodall similarity
goodall_similarity <- function(x, y, p) {
if (is.na(x) || is.na(y)) return(0)
if (x == y) return(1 - (p^2))
return(0)
}
# Calculate pk(x) for each categorical column
pk_values <- lapply(categorical_cols, function(col) {
table(securities_subset[[col]]) / nrow(securities_subset)
})
# empty matrix to store similarity scores
n <- nrow(securities_subset)
tickers <- as.character(securities_subset$`Ticker symbol`)
overlap_matrix <- matrix(0, n, n, dimnames=list(tickers, tickers))
inverse_frequency_matrix <- matrix(0, n, n, dimnames=list(tickers, tickers))
goodall_matrix <- matrix(0, n, n, dimnames=list(tickers, tickers))
# Calculate similarity for each pair of tickers
for (i in 1:n) {
for (j in 1:n) {
if (i == j) next # Skip diagonal values
for (col in 1:length(categorical_cols)) {
xi <- securities_subset[i, categorical_cols[col]]
yi <- securities_subset[j, categorical_cols[col]]
p <- pk_values[[col]][as.character(xi)]
overlap_matrix[i, j] <- overlap_matrix[i, j] + overlap_similarity(xi, yi)
inverse_frequency_matrix[i, j] <- inverse_frequency_matrix[i, j] + inverse_frequency_similarity(xi, yi, p)
goodall_matrix[i, j] <- goodall_matrix[i, j] + goodall_similarity(xi, yi, p)
}
}
}
# Convert matrices to data frames for easier sorting and viewing
overlap_df <- as.data.frame(as.table(overlap_matrix))
inverse_frequency_df <- as.data.frame(as.table(inverse_frequency_matrix))
goodall_df <- as.data.frame(as.table(goodall_matrix))
# Print top 10 and bottom 10 similarities, excluding diagonal and duplicate pairs
print("Top 10 Overlap Similarities")
print(head(overlap_df %>% filter(as.character(Var1) != as.character(Var2) & as.character(Var1) < as.character(Var2)) %>% arrange(desc(Freq)), 10))
print("Bottom 10 Overlap Similarities")
print(head(overlap_df %>% filter(as.character(Var1) != as.character(Var2) & as.character(Var1) < as.character(Var2)) %>% arrange(Freq), 10))
print("Top 10 Inverse Frequency Similarities")
print(head(inverse_frequency_df %>% filter(as.character(Var1) != as.character(Var2) & as.character(Var1) < as.character(Var2)) %>% arrange(desc(Freq)), 10))
print("Bottom 10 Inverse Frequency Similarities")
print(head(inverse_frequency_df %>% filter(as.character(Var1) != as.character(Var2) & as.character(Var1) < as.character(Var2)) %>% arrange(Freq), 10))
print("Top 10 Goodall Similarities")
print(head(goodall_df %>% filter(as.character(Var1) != as.character(Var2) & as.character(Var1) < as.character(Var2)) %>% arrange(desc(Freq)), 10))
print("Bottom 10 Goodall Similarities")
print(head(goodall_df %>% filter(as.character(Var1) != as.character(Var2) & as.character(Var1) < as.character(Var2)) %>% arrange(Freq), 10))
# Load necessary libraries
library(readr)
library(dplyr)
library(MASS)
library(stats)
# Load the data from the CSV files
fundamentals_df <- read_csv("fundamentals.csv")
quantitative_cols <- c("After Tax ROE", "Cash Ratio", "Current Ratio", "Operating Margin", "Pre-Tax Margin",
"Pre-Tax ROE", "Profit Margin", "Quick Ratio", "Total Assets", "Total Liabilities", "Earnings Per Share")
# Filter Data for Specific Year (2013)
fundamentals_2013 <- filter(fundamentals_df, substr(`Period Ending`, 1, 4) == "2013")
fundamentals_2013 <- fundamentals_2013 %>%
rowwise() %>%
mutate(missing_or_zero = sum(across(all_of(quantitative_cols), ~is.na(.) | . == 0), na.rm = TRUE)) %>%
ungroup()
# Sorting the DataFrame based on the missing_or_zero column and then picking the first 100 tickers.
top_100_tickers <- fundamentals_2013 %>%
arrange(missing_or_zero) %>%
head(100) %>%
pull('Ticker Symbol')
# Selecting rows for the top 100 tickers
fundamentals_2013_subset <- filter(fundamentals_2013, `Ticker Symbol` %in% top_100_tickers)
# Select only the quantitative columns
selected_columns <- c("Ticker Symbol", "After Tax ROE", "Cash Ratio", "Current Ratio", "Operating Margin", "Pre-Tax Margin", "Pre-Tax ROE", "Profit Margin", "Quick Ratio", "Total Assets", "Total Liabilities", "Earnings Per Share")
fundamentals_2013_subset_selected <- dplyr::select(fundamentals_2013_subset, all_of(selected_columns))
# Standardize the data (mean = 0, sd = 1)
# Exclude the 'Ticker Symbol' column when standardizing
fundamentals_2013_subset_selected_std <- fundamentals_2013_subset_selected
fundamentals_2013_subset_selected_std[-1] <- scale(fundamentals_2013_subset_selected[-1])
# Compute the covariance matrix and its inverse
cov_matrix <- cov(fundamentals_2013_subset_selected_std[-1])
inv_cov_matrix <- solve(cov_matrix)
# Function to compute Mahalanobis distance
mahalanobis_distance <- function(x, y, inv_cov_matrix) {
diff <- matrix(x - y, ncol = 1)  # Convert difference to column vector
dist <- sqrt(t(diff) %*% inv_cov_matrix %*% diff)
return(dist)
}
# Matrix to store Mahalanobis distances
mahalanobis_matrix <- matrix(NA,
nrow = nrow(fundamentals_2013_subset_selected_std),
ncol = nrow(fundamentals_2013_subset_selected_std))
rownames(mahalanobis_matrix) <- fundamentals_2013_subset_selected_std$`Ticker Symbol`
colnames(mahalanobis_matrix) <- fundamentals_2013_subset_selected_std$`Ticker Symbol`
# Compute Mahalanobis distance for each pair of tickers
for (i in 1:nrow(fundamentals_2013_subset_selected_std)) {
for (j in 1:nrow(fundamentals_2013_subset_selected_std)) {
x <- as.numeric(fundamentals_2013_subset_selected_std[i, -1])
y <- as.numeric(fundamentals_2013_subset_selected_std[j, -1])
mahalanobis_matrix[i, j] <- mahalanobis_distance(x, y, inv_cov_matrix)
}
}
mahalanobis_df <- as.data.frame(as.table(mahalanobis_matrix))
# Sort the distances
sorted_mahalanobis <- mahalanobis_df %>% arrange(Freq)
top_10_mahalanobis <- head(sorted_mahalanobis, 10)
bottom_10_mahalanobis <- tail(sorted_mahalanobis, 10)
# Display or save results
print("Top 10 Mahalanobis distances:")
print(top_10_mahalanobis)
print("Bottom 10 Mahalanobis distances:")
print(bottom_10_mahalanobis)
# Load the required libraries
library(readr)
library(dplyr)
library(reshape2)
# Load the data from the CSV files
fundamentals_df <- read_csv("fundamentals.csv")
securities_df <- read_csv("securities.csv")
# Filter Data for Specific Year (e.g., 2013)
fundamentals_2013 <- filter(fundamentals_df, substr(`Period Ending`, 1, 4) == "2013")
# Select tickers with least missing or zero values in quantitative columns
quantitative_cols <- c("After Tax ROE", "Cash Ratio", "Current Ratio", "Operating Margin", "Pre-Tax Margin",
"Pre-Tax ROE", "Profit Margin", "Quick Ratio", "Total Assets", "Total Liabilities", "Earnings Per Share")
fundamentals_2013 <- fundamentals_2013 %>%
rowwise() %>%
mutate(missing_or_zero = sum(across(all_of(quantitative_cols), ~is.na(.) | . == 0), na.rm = TRUE)) %>%
ungroup()
# Sorting the DataFrame based on the missing_or_zero column and then picking the first 100 tickers.
top_100_tickers <- fundamentals_2013 %>%
arrange(missing_or_zero) %>%
head(100) %>%
pull('Ticker Symbol')
# Selecting rows for the top 100 tickers
fundamentals_2013_subset <- filter(fundamentals_2013, `Ticker Symbol` %in% top_100_tickers)
securities_subset <- filter(securities_df, `Ticker symbol` %in% top_100_tickers)
# Select only the quantitative columns of interest from fundamentals_2013_subset
selected_columns <- c("Ticker Symbol", "After Tax ROE", "Cash Ratio", "Current Ratio", "Operating Margin", "Pre-Tax Margin", "Pre-Tax ROE", "Profit Margin", "Quick Ratio", "Total Assets", "Total Liabilities", "Earnings Per Share")
fundamentals_2013_subset_selected <- dplyr::select(fundamentals_2013_subset, all_of(selected_columns))
# Merge the two data frames
merged_df <- merge(fundamentals_2013_subset_selected, securities_subset, by.x = "Ticker Symbol", by.y = "Ticker symbol")
df_numeric <- merged_df[, c("After Tax ROE", "Cash Ratio", "Current Ratio", "Operating Margin", "Pre-Tax Margin",
"Pre-Tax ROE", "Profit Margin", "Quick Ratio", "Total Assets", "Total Liabilities", "Earnings Per Share")]
df_categorical <- merged_df[, c("Security", "SEC filings", "GICS Sector","GICS Sub Industry", "Address of Headquarters")]
lambda <- 0.8
# Function to calculate categorical similarity
calc_cat_sim <- function(x, y) {
return(ifelse(all(x == y), 1, 0))
}
calc_num_sim <- function(x, y) {
return(sum((x - y)^2))
}
num_tickers <- nrow(merged_df)
overall_sim_matrix <- matrix(0, nrow=num_tickers, ncol=num_tickers)
overall_sim_df <- data.frame(Var1 = character(), Var2 = character(), value = numeric())
ticker_symbols <- merged_df$`Ticker Symbol`
# overall similarity
for (i in 1:(num_tickers - 1)) {
for (j in (i + 1):num_tickers) {
if (i == j) {
overall_sim_matrix[i, j] <- 1 #similarity score of 1 when tickers are  same
} else {
num_sim <- calc_num_sim(df_numeric[i, ], df_numeric[j, ])
cat_sim <- calc_cat_sim(df_categorical[i, ], df_categorical[j, ])
overall_sim <- lambda * num_sim + (1 - lambda) * cat_sim
overall_sim_df <- rbind(overall_sim_df, data.frame(Var1 = ticker_symbols[i], Var2 = ticker_symbols[j], value = overall_sim))
}
}
}
ticker_symbols <- merged_df$`Ticker Symbol`
rownames(overall_sim_matrix) <- ticker_symbols
colnames(overall_sim_matrix) <- ticker_symbols
# Rank the similarities
overall_sim_df$rank <- rank(-overall_sim_df$value)
top_10_sim <- overall_sim_df[order(-overall_sim_df$value), ][1:10,]
bottom_10_sim <- overall_sim_df[order(overall_sim_df$value), ][1:10,]
similarity_results <- (list(top_10 = top_10_sim, bottom_10 = bottom_10_sim))
print("Top 10 similarities:")
print(similarity_results$top_10)
print("Bottom 10 similarities:")
print(similarity_results$bottom_10)
# Load the required libraries
library(readr)
library(dplyr)
library(reshape2)
# Load the data from the CSV files
fundamentals_df <- read_csv("fundamentals.csv")
securities_df <- read_csv("securities.csv")
# Filter Data for Specific Year (e.g., 2013)
fundamentals_2013 <- filter(fundamentals_df, substr(`Period Ending`, 1, 4) == "2013")
# Select tickers with least missing or zero values in quantitative columns
quantitative_cols <- c("After Tax ROE", "Cash Ratio", "Current Ratio", "Operating Margin", "Pre-Tax Margin",
"Pre-Tax ROE", "Profit Margin", "Quick Ratio", "Total Assets", "Total Liabilities", "Earnings Per Share")
fundamentals_2013 <- fundamentals_2013 %>%
rowwise() %>%
mutate(missing_or_zero = sum(across(all_of(quantitative_cols), ~is.na(.) | . == 0), na.rm = TRUE)) %>%
ungroup()
# Sorting the DataFrame based on the missing_or_zero column and then picking the first 100 tickers.
top_100_tickers <- fundamentals_2013 %>%
arrange(missing_or_zero) %>%
head(100) %>%
pull('Ticker Symbol')
# Selecting rows for the top 100 tickers
fundamentals_2013_subset <- filter(fundamentals_2013, `Ticker Symbol` %in% top_100_tickers)
securities_subset <- filter(securities_df, `Ticker symbol` %in% top_100_tickers)
# Select only the quantitative columns of interest from fundamentals_2013_subset
selected_columns <- c("Ticker Symbol", "After Tax ROE", "Cash Ratio", "Current Ratio", "Operating Margin", "Pre-Tax Margin", "Pre-Tax ROE", "Profit Margin", "Quick Ratio", "Total Assets", "Total Liabilities", "Earnings Per Share")
fundamentals_2013_subset_selected <- dplyr::select(fundamentals_2013_subset, all_of(selected_columns))
# Merge the two data frames
merged_df <- merge(fundamentals_2013_subset_selected, securities_subset, by.x = "Ticker Symbol", by.y = "Ticker symbol")
df_numeric <- merged_df[, c("After Tax ROE", "Cash Ratio", "Current Ratio", "Operating Margin", "Pre-Tax Margin",
"Pre-Tax ROE", "Profit Margin", "Quick Ratio", "Total Assets", "Total Liabilities", "Earnings Per Share")]
df_categorical <- merged_df[, c("Security", "SEC filings", "GICS Sector","GICS Sub Industry", "Address of Headquarters")]
# Define the function
compute_normalized_similarity <- function(df_numeric, df_categorical, lambda) {
# Function to calculate numerical similarity (e.g., Euclidean distance)
calc_num_sim <- function(x, y, sigma_n) {
return(sum(((x - y)^2) / sigma_n^2))
}
# Function to calculate categorical similarity (e.g., Jaccard similarity)
calc_cat_sim <- function(x, y, sigma_c) {
return(sum(x == y) / sigma_c)
}
# Get the number of tickers
num_tickers <- nrow(df_numeric)
ticker_symbols <- merged_df$`Ticker Symbol`
# Initialize an empty matrix to store overall similarity values
overall_sim_df <- data.frame(Var1 = character(), Var2 = character(), value = numeric())
overall_sim_matrix <- matrix(0, nrow=num_tickers, ncol=num_tickers)
rownames(overall_sim_matrix) <- ticker_symbols
colnames(overall_sim_matrix) <- ticker_symbols
# Standard deviations for normalization
sigma_n <- apply(df_numeric, 2, sd, na.rm = TRUE)  # column-wise standard deviation
sigma_c <- length(unique(as.vector(as.matrix(df_categorical))))  # unique categories for normalization
# Calculate overall similarity
for (i in 1:(num_tickers - 1)) {
for (j in (i + 1):num_tickers) {
num_sim <- calc_num_sim(df_numeric[i, ], df_numeric[j, ], sigma_n)
cat_sim <- calc_cat_sim(df_categorical[i, ], df_categorical[j, ], sigma_c)
overall_sim <- lambda * num_sim + (1 - lambda) * cat_sim
overall_sim_df <- rbind(overall_sim_df, data.frame(Var1 = ticker_symbols[i], Var2 = ticker_symbols[j], value = overall_sim))
}
}
# Rank the similarities
overall_sim_df$rank <- rank(-overall_sim_df$value)  # - sign to rank highest similarities first
# Extract top and bottom 10 similarities
top_10_sim <- overall_sim_df[order(-overall_sim_df$value), ][1:10,]
bottom_10_sim <- overall_sim_df[order(overall_sim_df$value), ][1:10,]
# Return a list containing top and bottom 10 similarities
return(list(top_10 = top_10_sim, bottom_10 = bottom_10_sim))
}
# Assuming lambda = 0.5 for equal weight to numerical and categorical features
lambda <- 0.1
# Get top and bottom 10 similarities
similarity_results <- compute_normalized_similarity(df_numeric, df_categorical, lambda)
# Print results
print("Top 10 similarities:")
print(similarity_results$top_10)
print("Bottom 10 similarities:")
print(similarity_results$bottom_10)
# Load the required libraries
library(readr)
library(dplyr)
# Load the data from the CSV files
fundamentals_df <- read_csv("fundamentals.csv")
securities_df <- read_csv("securities.csv")
print("Structure of fundamentals_df:")
str(fundamentals_df)
# Filter Data for Specific Year (e.g., 2013)
fundamentals_2013 <- filter(fundamentals_df, substr(`Period Ending`, 1, 4) == "2013")
# Select tickers with least missing or zero values in quantitative columns
quantitative_cols <- c("After Tax ROE", "Cash Ratio", "Current Ratio", "Operating Margin", "Pre-Tax Margin",
"Pre-Tax ROE", "Profit Margin", "Quick Ratio", "Total Assets", "Total Liabilities", "Earnings Per Share")
fundamentals_2013 <- fundamentals_2013 %>%
rowwise() %>%
mutate(missing_or_zero = sum(across(all_of(quantitative_cols), ~is.na(.) | . == 0), na.rm = TRUE)) %>%
ungroup()
# Sorting the DataFrame based on the missing_or_zero column and then picking the first 100 tickers.
top_100_tickers <- fundamentals_2013 %>%
arrange(missing_or_zero) %>%
head(100) %>%
pull('Ticker Symbol')
# Selecting rows for the top 100 tickers
fundamentals_2013_subset <- filter(fundamentals_2013, `Ticker Symbol` %in% top_100_tickers)
# Select only the quantitative columns of interest from fundamentals_2013_subset
selected_columns <- c("Ticker Symbol", "After Tax ROE", "Cash Ratio", "Current Ratio", "Operating Margin", "Pre-Tax Margin", "Pre-Tax ROE", "Profit Margin", "Quick Ratio", "Total Assets", "Total Liabilities", "Earnings Per Share")
fundamentals_2013_subset_selected <- dplyr::select(fundamentals_2013_subset, all_of(selected_columns))
bucketize <- function(column, n_buckets) {
breaks <- quantile(column, probs = seq(0, 1, length.out = n_buckets + 1))
cut(column, breaks = breaks, labels = FALSE, include.lowest = TRUE)
}
match_score <- function(row1, row2) {
sum(row1 == row2)
}
# Bucketize each column
bucketized_data <- as.data.frame(lapply(fundamentals_2013_subset_selected[-1], bucketize, n_buckets=3))
# match-based similarity matrix
n <- nrow(bucketized_data)
match_similarity_matrix <- matrix(0, n, n)
rownames(match_similarity_matrix) <- fundamentals_2013_subset_selected$`Ticker Symbol`
colnames(match_similarity_matrix) <- fundamentals_2013_subset_selected$`Ticker Symbol`
for (i in 1:(n - 1)) {
for (j in (i + 1):n) {
score <- match_score(bucketized_data[i,], bucketized_data[j,])
match_similarity_matrix[i, j] <- score
match_similarity_matrix[j, i] <- score
}
}
match_similarity_df <- as.data.frame(as.table(match_similarity_matrix))
# Filter out the diagonal and duplicate entries
match_similarity_df <- match_similarity_df %>% filter(Var1 != Var2)
# Sort by similarity score (Freq) in descending order for top 10 pairs
sorted_df_top <- match_similarity_df %>% arrange(desc(Freq))
# Print out the top 10 most similar pairs
print("Top 10 most similar pairs based on match score:")
print(head(sorted_df_top, 10))
# Sort by similarity score (Freq) in ascending order for bottom 10 pairs
sorted_df_bottom <- match_similarity_df %>% arrange(Freq)
# Print out the bottom 10 least similar pairs
print("Bottom 10 least similar pairs based on match score:")
print(head(sorted_df_bottom, 10))
